{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit Stress Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNtIw1u+BUp/MqpWb7dM8Ii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CagataySencan/StressAnalysis/blob/main/Reddit_Stress_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppor Vector Machine\n"
      ],
      "metadata": {
        "id": "tmir7qcNsOOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "ep_V0GQog5rb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import  GaussianNB\n",
        "from sklearn.metrics import  confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import metrics\n",
        "import seaborn as sb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "id": "E0TEfZEPh1fi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "qfKK59yQh3Ul"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "-la-iWRXh5aT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "qF3IyGhNh6wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download ruchi798/stress-analysis-in-social-media"
      ],
      "metadata": {
        "id": "PWxScyObh9e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip stress-analysis-in-social-media.zip"
      ],
      "metadata": {
        "id": "OqZTGkZOiGnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainData = pd.read_csv(\"dreaddit-train.csv\")\n",
        "testData = pd.read_csv((\"dreaddit-test.csv\"))"
      ],
      "metadata": {
        "id": "KiWwj6-YiTh_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eksik veri kontrolü\n",
        "print(\"-Eğitim verisi-\\n\",trainData.isnull().sum(),\"\\n-------------\\n\",\"-Test Verisi-\\n\",testData.isnull().sum())"
      ],
      "metadata": {
        "id": "CL-RbA5Hie_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ve test datalarımızı birleştirdik\n",
        "X_Train = pd.DataFrame(trainData['text'], columns = ['text'])\n",
        "X_Test = pd.DataFrame(testData['text'], columns = ['text'])\n",
        "X = pd.concat((X_Train, X_Test), sort=False).reset_index(drop=True)\n",
        "\n",
        "Y_Train =  pd.DataFrame(trainData['subreddit'], columns = ['subreddit'])\n",
        "Y_Test =  pd.DataFrame(testData['subreddit'], columns = ['subreddit'])\n",
        "Y =  pd.concat((Y_Train, Y_Test), sort=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "QoFsejv2kgfT"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Önişleme\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "S6RtCh2Fr_TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Küçük harfe çevirme, stopwordslerden ve noktalama işaretlerinden temizleme\n",
        "for i in range(X['text'].count()) : \n",
        "   \n",
        "   text = X['text'][i]\n",
        "   text = text.lower()\n",
        "   text = remove_stopwords(text)\n",
        "   text = text.translate(str.maketrans('','', punctuation))\n",
        "   X['text'][i] = text\n",
        "\n",
        "# Common wordlerden temizleme\n",
        "cnt = Counter()\n",
        "for text in X['text'].values : \n",
        "  for word in text.split() :\n",
        "    cnt[word] += 1\n",
        "\n",
        "cnt.most_common(10)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-skuxgElu1uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = set([w for (w, wc) in cnt.most_common(10)])\n",
        "def removeFreqwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in freq])\n",
        "\n",
        "X['text'] = X['text'].apply(lambda text: removeFreqwords(text))"
      ],
      "metadata": {
        "id": "97xvo9zz_17Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmalama işlemi\n",
        "\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizer(text):\n",
        "    return \" \".join([lm.lemmatize(word) for word in text.split()])\n",
        "\n",
        "X['text'] = X['text'].apply(lambda text: lemmatizer(text))\n"
      ],
      "metadata": {
        "id": "Mpb_bVVWDahy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoder ile subreddit label oluşturma\n",
        "le = LabelEncoder()\n",
        "subreddit_id = pd.DataFrame(le.fit_transform(Y), columns = ['subreddit_id'])\n",
        "X['subreddit_id'] = subreddit_id\n"
      ],
      "metadata": {
        "id": "sK1ocXM8KQop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vektörleme işlemi\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
        "                        ngram_range=(1, 2), \n",
        "                        stop_words='english')\n",
        "\n",
        "features = tfidf.fit_transform(X.text).toarray()\n",
        "\n",
        "labels = X.subreddit_id"
      ],
      "metadata": {
        "id": "2VAfBf4ELr5q"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.25,random_state = 20)"
      ],
      "metadata": {
        "id": "R7FzG2fvRO1d"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearSVC()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Karmaşıklık matrisi :\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Doğruluk skoru : \\n\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Recall\n",
        "from sklearn.metrics import recall_score\n",
        "print(\"Duyarlılık skoru : \\n\",recall_score(y_test, y_pred, average=None))\n",
        "\n",
        "# Precision\n",
        "from sklearn.metrics import precision_score\n",
        "print(\"Kesinlik skoru : \\n\",precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "id": "95mKrkU6S1va",
        "outputId": "c0ccc054-40d7-4fd5-d441-df2be7006e12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Karmaşıklık matrisi :\n",
            " [[  4   3   8   1   0   6   7   3   0   0]\n",
            " [  0  97   8   8   0   2  25  23   0   4]\n",
            " [  2   5  48   4   0   5   5  10   0   1]\n",
            " [  2   3   7  31   0   0  26  19   1   6]\n",
            " [  0   0   2   0   3   1   1   0   0   0]\n",
            " [  2   4   5   3   0  28   5   5   0   3]\n",
            " [  0  23   4   6   0   2 100  23   0   6]\n",
            " [  0  17   4  11   0   0  19 131   1   3]\n",
            " [  1   7   4   0   0   0   5   6   2   3]\n",
            " [  0   1   4   8   0   0  20  21   1  20]]\n",
            "Doğruluk skoru : \n",
            " 0.5219347581552306\n",
            "Duyarlılık skoru : \n",
            " [0.125      0.58083832 0.6        0.32631579 0.42857143 0.50909091\n",
            " 0.6097561  0.70430108 0.07142857 0.26666667]\n",
            "Kesinlik skoru : \n",
            " [0.36363636 0.60625    0.5106383  0.43055556 1.         0.63636364\n",
            " 0.46948357 0.54356846 0.4        0.43478261]\n"
          ]
        }
      ]
    }
  ]
}